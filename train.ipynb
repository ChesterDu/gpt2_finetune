{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_kIArPDCDmmv",
        "qC_cTxEu_jSs",
        "NFJoqdZH_w_y",
        "VHf3kVeNDIzG",
        "N3WGc3axEmYD"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCarLbMXDvy5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "c0d96cdb-3e4c-4001-cb55-4f0147f478d0"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (3.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (3.2.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.34.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 37.5MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.18.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.12.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.29.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15.0) (47.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=d9fa98d26296925fac9a3694a28f726fb14725be70ea19db5d037e4f65290d18\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement tensorflow-estimator<2.3.0,>=2.2.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, gast, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsJ4PizYF8jw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "9adb8a78-d1c0-4bf1-d75d-c1b4e4dfa53e"
      },
      "source": [
        "!pip install -q gpt_2_simple\n",
        "import gpt_2_simple as gpt2\n",
        "gpt2.download_gpt2(model_name=\"117M\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 531Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 98.5Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 364Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 186Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 259Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 134Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 131Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfv6YyeIAFlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import regex as re\n",
        "from functools import lru_cache\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.contrib.training import HParams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj1eoz6UC4wU",
        "colab_type": "text"
      },
      "source": [
        "## Flags and Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Xo71vUAgC2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ecd2564-78b2-45a8-a853-0f45e9d1e224"
      },
      "source": [
        "class FLAGS():\n",
        "  def __init__(self, is_train = 1, cond = 1, model_dir = None, gpu = '0', batch_size = 10, data_name = 'kg', n_class = 4, learning_rate = 1e-4, data_dir = \"data\",length = 200, temperature = 0.7, top_k = 40):\n",
        "    self.is_train = bool(is_train)\n",
        "    self.cond = bool(cond)\n",
        "    self.model_dir = model_dir\n",
        "    self.gpu = gpu\n",
        "    self.batch_size = batch_size\n",
        "    self.data_name = data_name\n",
        "    self.n_class = n_class\n",
        "    self.learning_rate = 1e-4\n",
        "    self.data_dir = data_dir\n",
        "    self.length = length\n",
        "    self.temperature = temperature\n",
        "    self.top_k = top_k\n",
        "\n",
        "FLAGS = FLAGS(model_dir = 'models/117M',batch_size = 50)\n",
        "FLAGS.is_train"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D2CJuihCnXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = os.path.expanduser(os.path.expandvars(FLAGS.model_dir))\n",
        "enc = get_encoder(model_dir)\n",
        "PAD_ID = enc.encoder['<|endoftext|>']\n",
        "hparams = default_hparams()\n",
        "with open(os.path.join(model_dir, 'hparams.json')) as f:\n",
        "    hparams.override_from_dict(json.load(f))\n",
        "\n",
        "def load_data(path, fname, enc, label):\n",
        "    data = []\n",
        "    print('loading %s/%s ......' % (path, fname))\n",
        "    with open('%s/%s.txt' % (path, fname)) as f:\n",
        "        tmp = []\n",
        "        for k, line in enumerate(f):\n",
        "            i = k + 1\n",
        "            if i % 6 == 0:\n",
        "                data.append({\"st\": tmp, \"label\": label})\n",
        "                tmp = []\n",
        "            else:\n",
        "                tmp.append(enc.encode(line.strip().replace(\" .\", \". \")))\n",
        "    return data\n",
        "\n",
        "def load_data_kg(path, fname, enc):\n",
        "    with open(\"%s/%s.txt\" % (path, fname)) as fin:\n",
        "        data = [enc.encode(line.strip()) for line in fin]\n",
        "    return data\n",
        "\n",
        "def padding(sent, l):\n",
        "    return sent + [PAD_ID] * (l-len(sent))\n",
        "\n",
        "# def gen_batched_data(data):\n",
        "#     max_story_len = max([sum([len(item[\"st\"][i]) for i in range(5)]) for item in data]) + 1\n",
        "#     max_input_story_len = max([len(item[\"st\"][0]) for item in data]) + 1\n",
        "#     story, story_length, label = [], [], []\n",
        "#     input_story, input_story_length = [], []\n",
        "\n",
        "#     for item in data:\n",
        "#         input_story.append(padding(item[\"st\"][0], max_input_story_len))\n",
        "#         input_story_length.append(len(item[\"st\"][0]) + 1)    \n",
        "#         story.append([])\n",
        "#         for i in range(5):\n",
        "#             story[-1] += item[\"st\"][i]\n",
        "#         story_length.append(len(story[-1]) + 1)\n",
        "#         story[-1] = padding(story[-1], max_story_len)\n",
        "#         label.append(item[\"label\"])\n",
        "#     batched_data = {\n",
        "#         \"story\": np.array(story),\n",
        "#         \"story_length\": np.array(story_length),\n",
        "#         \"input_story\": np.array(input_story),\n",
        "#         \"input_story_length\": np.array(input_story_length),\n",
        "#         \"label\": np.array(label),\n",
        "#     }\n",
        "#     return batched_data\n",
        "\n",
        "\n",
        "def gen_batched_data_from_kg(data):\n",
        "    max_story_len = max([len(item) for item in data]) + 1\n",
        "    story, story_length = [], []\n",
        "    for item in data:\n",
        "        story.append(padding(item, max_story_len))\n",
        "        story_length.append(len(item) + 1)\n",
        "    batched_data = {\n",
        "        \"story\": np.array(story),\n",
        "        \"story_length\": np.array(story_length),\n",
        "    }\n",
        "    return batched_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kIArPDCDmmv",
        "colab_type": "text"
      },
      "source": [
        "## Models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXl-NySbDo1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def default_hparams():\n",
        "    return HParams(\n",
        "        n_vocab=0,\n",
        "        n_ctx=1024,\n",
        "        n_embd=768,\n",
        "        n_head=12,\n",
        "        n_layer=12,\n",
        "    )\n",
        "\n",
        "def shape_list(x):\n",
        "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
        "    static = x.shape.as_list()\n",
        "    dynamic = tf.shape(x)\n",
        "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    x = x - tf.reduce_max(x, axis=axis, keepdims=True)\n",
        "    ex = tf.exp(x)\n",
        "    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
        "\n",
        "def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
        "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
        "    with tf.variable_scope(scope):\n",
        "        n_state = x.shape[-1].value\n",
        "        g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1))\n",
        "        b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0))\n",
        "        u = tf.reduce_mean(x, axis=axis, keepdims=True)\n",
        "        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n",
        "        x = (x - u) * tf.rsqrt(s + epsilon)\n",
        "        x = x*g + b\n",
        "        return x\n",
        "\n",
        "def split_states(x, n):\n",
        "    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
        "    *start, m = shape_list(x)\n",
        "    return tf.reshape(x, start + [n, m//n])\n",
        "\n",
        "def merge_states(x):\n",
        "    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
        "    *start, a, b = shape_list(x)\n",
        "    return tf.reshape(x, start + [a*b])\n",
        "\n",
        "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
        "    with tf.variable_scope(scope):\n",
        "        *start, nx = shape_list(x)\n",
        "        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n",
        "        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))\n",
        "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
        "        return c\n",
        "\n",
        "def attention_mask(nd, ns, *, dtype):\n",
        "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
        "\n",
        "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
        "    \"\"\"\n",
        "    i = tf.range(nd)[:,None]\n",
        "    j = tf.range(ns)\n",
        "    m = i >= (j - ns + nd)\n",
        "    return tf.cast(m, dtype)\n",
        "\n",
        "def attention_mask_s2s(nd, ns, *, dtype):\n",
        "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
        "\n",
        "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
        "    \"\"\"\n",
        "    i = tf.range(nd)[:,None]\n",
        "    j = tf.range(ns)\n",
        "    m = i >= (j - ns + nd)\n",
        "    return tf.cast(m, dtype)\n",
        "\n",
        "\n",
        "def attn(x, scope, n_state, *, past, hparams):\n",
        "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
        "    assert n_state % hparams.n_head == 0\n",
        "    if past is not None:\n",
        "        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
        "\n",
        "    def split_heads(x):\n",
        "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
        "        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n",
        "\n",
        "    def merge_heads(x):\n",
        "        # Reverse of split_heads\n",
        "        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
        "\n",
        "    def mask_attn_weights(w):\n",
        "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
        "        batch_size, _, nd, ns = shape_list(w)\n",
        "        b = attention_mask(nd, ns, dtype=w.dtype,)\n",
        "        b = tf.reshape(b, [1, 1, nd, ns])\n",
        "        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
        "        return w\n",
        "\n",
        "    def multihead_attn(q, k, v):\n",
        "        # q, k, v have shape [batch, heads, sequence, features]\n",
        "        w = tf.matmul(q, k, transpose_b=True)\n",
        "        # w = tf.Print(w, [tf.shape(q), tf.shape(k), tf.shape(v), tf.shape(w)], summarize=1e5)\n",
        "        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n",
        "\n",
        "        w = mask_attn_weights(w)\n",
        "        w = softmax(w)\n",
        "        a = tf.matmul(w, v)\n",
        "        return a\n",
        "\n",
        "    with tf.variable_scope(scope):\n",
        "        c = conv1d(x, 'c_attn', n_state*3)\n",
        "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
        "        present = tf.stack([k, v], axis=1)\n",
        "        if past is not None:\n",
        "            pk, pv = tf.unstack(past, axis=1)\n",
        "            k = tf.concat([pk, k], axis=-2)\n",
        "            v = tf.concat([pv, v], axis=-2)\n",
        "        a = multihead_attn(q, k, v)\n",
        "        a = merge_heads(a)\n",
        "        a = conv1d(a, 'c_proj', n_state)\n",
        "        return a, present\n",
        "\n",
        "\n",
        "def mlp(x, scope, n_state, *, hparams):\n",
        "    with tf.variable_scope(scope):\n",
        "        nx = x.shape[-1].value\n",
        "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
        "        h2 = conv1d(h, 'c_proj', nx)\n",
        "        return h2\n",
        "\n",
        "\n",
        "def block(x, scope, *, past, hparams):\n",
        "    with tf.variable_scope(scope):\n",
        "        nx = x.shape[-1].value\n",
        "        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
        "        x = x + a\n",
        "        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
        "        x = x + m\n",
        "        return x, present\n",
        "\n",
        "def past_shape(*, hparams, batch_size=None, sequence=None):\n",
        "    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n",
        "\n",
        "def expand_tile(value, size):\n",
        "    \"\"\"Add a new axis of given size.\"\"\"\n",
        "    value = tf.convert_to_tensor(value, name='value')\n",
        "    ndims = value.shape.ndims\n",
        "    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
        "\n",
        "def positions_for(tokens, past_length):\n",
        "    batch_size = tf.shape(tokens)[0]\n",
        "    nsteps = tf.shape(tokens)[1]\n",
        "    return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
        "\n",
        "def transformer(h, hparams, past):\n",
        "    presents = []\n",
        "    pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
        "    assert len(pasts) == hparams.n_layer\n",
        "    for layer, past in enumerate(pasts):\n",
        "        h, present = block(h, 'h%d' % (layer), past=past, hparams=hparams)\n",
        "        presents.append(present)\n",
        "    return h, presents\n",
        "\n",
        "def model(hparams, X, past=None, kg=None, kg_mask=None, kg_past=None, scope='model', reuse=False):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        results = {}\n",
        "        batch, sequence = shape_list(X)\n",
        "        wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
        "                             initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
        "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
        "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
        "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
        "        print_op = tf.print([\"x:\", batch, sequence, tf.shape(h)])\n",
        "        with tf.control_dependencies([]):\n",
        "            # Transformer\n",
        "            h, presents = transformer(h, hparams, past)\n",
        "        results['present'] = tf.stack(presents, axis=1)\n",
        "        h = norm(h, 'ln_f')\n",
        "\n",
        "    # Language model loss.  Do tokens <n predict token n?\n",
        "    h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
        "    results['hidden_state'] = tf.reshape(h_flat, [batch, sequence, hparams.n_embd])\n",
        "    logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
        "    logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
        "    results['logits'] = logits\n",
        "    return results\n",
        "\n",
        "def model_train(*, hparams, context, context_length, enc, PAD_ID):\n",
        "    batch_size = tf.shape(context)[0]\n",
        "    decoder_length = tf.shape(context)[1]\n",
        "\n",
        "    context_input = tf.concat([tf.ones([batch_size, 1], dtype=tf.int32)*PAD_ID,\n",
        "        tf.split(context, [decoder_length-1, 1], 1)[0]], 1)   # batch*len\n",
        "    context_mask = tf.reshape(tf.cumsum(tf.one_hot(context_length - 1,\n",
        "        decoder_length), reverse=True, axis=1), [-1, decoder_length])\n",
        "\n",
        "    lm_output = model(hparams=hparams, X=context_input, past=None, reuse=tf.AUTO_REUSE)\n",
        "    logits = lm_output[\"logits\"]\n",
        "    labels_onehot = tf.clip_by_value(tf.one_hot(context, hparams.n_vocab), 0.0, 1.0)\n",
        "    local_loss = tf.reduce_sum(-labels_onehot * tf.nn.log_softmax(logits), 2) * context_mask\n",
        "    loss = tf.reduce_sum(local_loss)\n",
        "    total_size = tf.reduce_sum(context_mask)\n",
        "    total_size += 1e-12 # to avoid division by 0 for all-0 weights\n",
        "    return loss / total_size\n",
        "\n",
        "def train_classify(*, hparams, context, context_length, label, enc, n_class, PAD_ID):\n",
        "    batch_size = tf.shape(context)[0]\n",
        "    decoder_length = tf.shape(context)[1]\n",
        "\n",
        "    context_input = tf.concat([tf.ones([batch_size, 1], dtype=tf.int32)*PAD_ID,\n",
        "        tf.split(context, [decoder_length-1, 1], 1)[0]], 1)   # batch*len\n",
        "    context_mask = tf.reshape(tf.cumsum(tf.one_hot(context_length-1,\n",
        "        decoder_length), reverse=True, axis=1), [-1, decoder_length])\n",
        "\n",
        "    lm_output = model(hparams=hparams, X=context_input, past=None, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "    logits = tf.reduce_sum(lm_output[\"hidden_state\"]*tf.expand_dims(context_mask, 2), 1) / (tf.reduce_sum(tf.expand_dims(context_mask, 2), 1) + 1e-12)\n",
        "    logits = tf.layers.dense(logits, 4, name=\"fine_tuning/classify\")\n",
        "    local_loss = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(label, 4), logits=logits)\n",
        "    loss = tf.reduce_sum(local_loss)\n",
        "\n",
        "    acc_list = []\n",
        "    for l in range(n_class):\n",
        "        tmp_acc_mask = tf.cast(tf.equal(label, l), tf.float32)\n",
        "        for tmpl in range(n_class):\n",
        "            tmp_is_equal = tf.cast(tf.equal(tf.cast(tf.argmax(logits, 1), tf.int32), tmpl), tf.float32)\n",
        "            acc_list.append(tf.reduce_sum(tmp_is_equal*tmp_acc_mask) / (tf.reduce_sum(tmp_acc_mask)+1e-12))\n",
        "\n",
        "    labels_onehot = tf.clip_by_value(tf.one_hot(context, hparams.n_vocab), 0.0, 1.0)\n",
        "    # [batch_size, 1]\n",
        "    label_mask = tf.expand_dims(tf.cast(tf.equal(label, 0), tf.float32), 1)\n",
        "    logits = lm_output[\"logits\"]\n",
        "    local_loss_lm = tf.reduce_sum(-labels_onehot * tf.nn.log_softmax(logits), 2) * context_mask * label_mask\n",
        "    loss_lm = tf.reduce_sum(local_loss_lm)\n",
        "    total_size = tf.reduce_sum(context_mask * label_mask)\n",
        "    total_size += 1e-12 # to avoid division by 0 for all-0 weights\n",
        "    loss_lm /= total_size\n",
        "    return 0.05*loss+loss_lm, loss_lm, acc_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC_cTxEu_jSs",
        "colab_type": "text"
      },
      "source": [
        "##Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHljbOap_mmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Byte pair encoding utilities\"\"\"\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
        "        self.errors = errors # how to handle errors in decoding\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "\n",
        "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
        "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        for token in re.findall(self.pat, text):\n",
        "            # print(\"before processing:\", token)\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            # print(\"after processing:\", token, self.bpe(token), [self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' ')])\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens, trunct=False):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
        "        if trunct:\n",
        "            text = text[:text.find('<|endoftext|>')]\n",
        "        return text\n",
        "\n",
        "def get_encoder(model_dir):\n",
        "    with open(os.path.join(model_dir, 'encoder.json'), 'r') as f:\n",
        "        encoder = json.load(f)\n",
        "    with open(os.path.join(model_dir, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
        "    return Encoder(\n",
        "        encoder=encoder,\n",
        "        bpe_merges=bpe_merges,\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFJoqdZH_w_y",
        "colab_type": "text"
      },
      "source": [
        "## generate_unconditional_sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5_P-Cc2_8xO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_model(\n",
        "    sess,\n",
        "    enc,\n",
        "    PAD_ID,\n",
        "    hparams,\n",
        "    temperature=1,\n",
        "    top_k=0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run the sample_model\n",
        "    :model_name=117M : String, which model to use\n",
        "    :seed=None : Integer seed for random number generators, fix seed to\n",
        "     reproduce results\n",
        "    :nsamples=0 : Number of samples to return, if 0, continues to\n",
        "     generate samples indefinately.\n",
        "    :batch_size=1 : Number of batches (only affects speed/memory).\n",
        "    :length=None : Number of tokens in generated text, if None (default), is\n",
        "     determined by model hyperparameters\n",
        "    :temperature=1 : Float value controlling randomness in boltzmann\n",
        "     distribution. Lower temperature results in less random completions. As the\n",
        "     temperature approaches zero, the model will become deterministic and\n",
        "     repetitive. Higher temperature results in more random completions.\n",
        "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "     considered for each step (token), resulting in deterministic completions,\n",
        "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "     special setting meaning no restrictions. 40 generally is a good value.\n",
        "     :models_dir : path to parent folder containing model subfolders\n",
        "     (i.e. contains the <model_name> folder)\n",
        "    \"\"\"\n",
        "\n",
        "    length = hparams.n_ctx // 2 if FLAGS.length is None else FLAGS.length\n",
        "    if length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    output = sample.sample_sequence(\n",
        "        hparams=hparams, length=length,\n",
        "        start_token=PAD_ID,\n",
        "        batch_size=1,\n",
        "        temperature=temperature, top_k=top_k\n",
        "    )[:, 1:]\n",
        "\n",
        "    generated = 0\n",
        "    while generated < 1000:\n",
        "        out = sess.run(output)\n",
        "        generated += 1\n",
        "        text = enc.decode(out[0], trunct=True)\n",
        "        print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "        print(text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHf3kVeNDIzG",
        "colab_type": "text"
      },
      "source": [
        "## Interactive generate samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02PjaMILDOdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interact_model(\n",
        "    sess,\n",
        "    enc,\n",
        "    PAD_ID,\n",
        "    hparams,\n",
        "    context,\n",
        "    dataset=None,\n",
        "    output_file_name=None,\n",
        "    relation=None,\n",
        "    temperature=1,\n",
        "    top_k=0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Interactively run the model\n",
        "    :model_name=117M : String, which model to use\n",
        "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
        "     results\n",
        "    :length=None : Number of tokens in generated text, if None (default), is\n",
        "     determined by model hyperparameters\n",
        "    :temperature=1 : Float value controlling randomness in boltzmann\n",
        "     distribution. Lower temperature results in less random completions. As the\n",
        "     temperature approaches zero, the model will become deterministic and\n",
        "     repetitive. Higher temperature results in more random completions.\n",
        "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "     considered for each step (token), resulting in deterministic completions,\n",
        "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "     special setting meaning no restrictions. 40 generally is a good value.\n",
        "     :models_dir : path to parent folder containing model subfolders\n",
        "     (i.e. contains the <model_name> folder)     \n",
        "    \"\"\"\n",
        "\n",
        "    if FLAGS.length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "    batch_size = 1\n",
        "    output = sample.sample_sequence(\n",
        "        hparams=hparams, length=FLAGS.length,\n",
        "        context=context,\n",
        "        start_token=PAD_ID,\n",
        "        batch_size=1,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "    )\n",
        "    if dataset is None:\n",
        "        while True:\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "            while not raw_text:\n",
        "                print('Prompt should not be empty!')\n",
        "                raw_text = input(\"Model prompt >>> \")\n",
        "            context_tokens = enc.encode(raw_text)\n",
        "            out = sess.run(output, feed_dict={\n",
        "                context: [context_tokens for _ in range(batch_size)]\n",
        "            })[:, len(context_tokens):]\n",
        "            text = enc.decode(out[0], trunct=True).strip()\n",
        "            print(text)\n",
        "            print(\"=\" * 80)\n",
        "    else:\n",
        "        fout = open(output_file_name, \"w\")\n",
        "        st, ed = 0, 0\n",
        "        while ed < len(dataset):\n",
        "            st, ed = ed, ed + 1\n",
        "            context_tokens = [[PAD_ID] + dataset[st][\"st\"][0]]\n",
        "            out = sess.run(output, feed_dict={\n",
        "                context: context_tokens,\n",
        "            })\n",
        "            for ipt, opt in zip(context_tokens, out):\n",
        "                opt = enc.decode(opt[len(ipt):], trunct=True)\n",
        "                ipt = enc.decode(ipt[1:], trunct=False)\n",
        "                fout.write(\"ipt: \" + ipt + \"\\n\")\n",
        "                fout.write(\"opt: \" + opt + \"\\n\")\n",
        "                fout.write(\"-\"*5+\"\\n\")\n",
        "        fout.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3WGc3axEmYD",
        "colab_type": "text"
      },
      "source": [
        "## sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osygpyU2EsOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_k_logits(logits, k):\n",
        "    if k == 0:\n",
        "        # no truncation\n",
        "        return logits\n",
        "\n",
        "    def _top_k():\n",
        "        values, _ = tf.nn.top_k(logits, k=k)\n",
        "        min_values = values[:, -1, tf.newaxis]\n",
        "        return tf.where(\n",
        "            logits < min_values,\n",
        "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
        "            logits,\n",
        "        )\n",
        "    return tf.cond(\n",
        "       tf.equal(k, 0),\n",
        "       lambda: logits,\n",
        "       lambda: _top_k(),\n",
        "    )\n",
        "\n",
        "\n",
        "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0):\n",
        "    if context is not None:\n",
        "        context_input = context\n",
        "        encoder_length = tf.shape(context)[1]\n",
        "    else:\n",
        "        context_input = tf.fill([batch_size, 1], start_token)\n",
        "\n",
        "    def step(hparams, tokens, past=None):\n",
        "        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
        "        presents = lm_output['present']\n",
        "        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'presents': presents,\n",
        "        }\n",
        "\n",
        "    with tf.name_scope('sample_sequence'):\n",
        "        def body(past, prev, output):\n",
        "            next_outputs = step(hparams, prev, past=past)\n",
        "            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)\n",
        "            logits = top_k_logits(logits, k=top_k)\n",
        "            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n",
        "            return [\n",
        "                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
        "                samples,\n",
        "                tf.concat([output, samples], axis=1)\n",
        "            ]\n",
        "        # past: the past hidden states of the transformer\n",
        "        # prev: the prediction token\n",
        "        # output: the cumulative tokens\n",
        "        past, prev, output = body(None, context_input, context_input)\n",
        "        prev = tf.reshape(prev, [batch_size, 1])\n",
        "        output = tf.reshape(output, [batch_size, -1])\n",
        "\n",
        "        def cond(*args):\n",
        "            return True\n",
        "\n",
        "        _, _, tokens = tf.while_loop(\n",
        "            cond=cond, body=body,\n",
        "            maximum_iterations=length - 1,\n",
        "            loop_vars=[\n",
        "                past,\n",
        "                prev,\n",
        "                output\n",
        "            ],\n",
        "            shape_invariants=[\n",
        "                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),\n",
        "                tf.TensorShape([batch_size, None]),\n",
        "                tf.TensorShape([batch_size, None]),\n",
        "            ],\n",
        "            back_prop=False,\n",
        "        )\n",
        "\n",
        "        return tokens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eQX633dE10H",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Soyb_aIvE0uL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8174a940-2adb-479b-d4cf-15c601390584"
      },
      "source": [
        "import time\n",
        "import copy\n",
        "import random\n",
        "import sys\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = FLAGS.gpu\n",
        "print(\"Using %s-th gpu ...\" % os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
        "train_dir = os.path.join(FLAGS.model_dir)\n",
        "assert os.path.exists(train_dir)\n",
        "\n",
        "def train(sess, dataset, is_train=True):\n",
        "    def pro_acc(acc):\n",
        "        final_acc = [[] for _ in range(FLAGS.n_class)]\n",
        "        for ac in acc:\n",
        "            for i in range(4):\n",
        "                if np.sum(ac[i*4:(i+1)*4]) == 1:\n",
        "                    final_acc[i].append(ac[i*4:(i+1)*4])\n",
        "        for i in range(4):\n",
        "            print(\"final classification confusion matrix (%d-th category):\" % i, np.mean(final_acc[i], 0).tolist())\n",
        "    st, ed = 0, 0\n",
        "    loss, loss_lm, acc = [], [], []\n",
        "    while ed < len(dataset):\n",
        "        if is_train:\n",
        "            output_feed = [\n",
        "                model_loss,\n",
        "                gradient_norm,\n",
        "                update,\n",
        "            ]\n",
        "        else:\n",
        "            output_feed = [\n",
        "                model_loss,\n",
        "            ]\n",
        "        st, ed = ed, ed + FLAGS.batch_size if ed + \\\n",
        "            FLAGS.batch_size < len(dataset) else len(dataset)\n",
        "        if FLAGS.data_name == \"multi_roc\" or FLAGS.data_name == \"roc\":\n",
        "            batch_data = gen_batched_data(dataset[st:ed])\n",
        "            input_feed = {\n",
        "                context: batch_data[\"story\"],\n",
        "                context_length: batch_data[\"story_length\"],\n",
        "                label: batch_data[\"label\"],\n",
        "            }\n",
        "            if FLAGS.data_name == \"multi_roc\": \n",
        "                output_feed.append(model_loss_lm)\n",
        "                output_feed.append(model_acc_list)\n",
        "        elif FLAGS.data_name == \"kg\":\n",
        "            batch_data = gen_batched_data_from_kg(dataset[st:ed])\n",
        "            input_feed = {\n",
        "                context: batch_data[\"story\"],\n",
        "                context_length: batch_data[\"story_length\"],\n",
        "            }\n",
        "        else:\n",
        "            print(\"DATANAME ERROR\")\n",
        "        outputs = sess.run(output_feed, input_feed)\n",
        "        loss.append(outputs[0])\n",
        "        if FLAGS.data_name == \"multi_roc\":\n",
        "            loss_lm.append(outputs[-2])\n",
        "            acc.append(outputs[-1])\n",
        "            if (st+1) % 10000 == 0:\n",
        "                print(\"current ppl:%.4f\"%np.exp(np.mean(loss_lm)))\n",
        "                pro_acc(acc)\n",
        "                print(\"=\"*5)\n",
        "    if is_train:\n",
        "        sess.run(epoch_add_op)\n",
        "    if FLAGS.data_name == \"multi_roc\":\n",
        "        pro_acc(acc)\n",
        "        return np.exp(np.mean(loss_lm))\n",
        "    else:\n",
        "        return np.exp(np.mean(loss))\n",
        "\n",
        "#**********************************************************************************\n",
        "#**********************************************************************************\n",
        "#**********************************************************************************\n",
        "\n",
        "print(\"begin loading dataset......\")\n",
        "if FLAGS.data_name == \"roc\":\n",
        "    data = load_data(FLAGS.data_dir, FLAGS.data_name, enc, label=0)\n",
        "    data_segnum = len(data) / 20\n",
        "    data_train = data[:int(data_segnum*18)]\n",
        "    data_dev = data[int(data_segnum*18):int(data_segnum*19)]\n",
        "    data_test = data[int(data_segnum*19):]\n",
        "elif FLAGS.data_name == \"kg\":\n",
        "    data_train = load_data_kg(FLAGS.data_dir, 'train', enc)\n",
        "    data_dev = load_data_kg(FLAGS.data_dir, 'valid', enc)\n",
        "    data_test = load_data_kg(FLAGS.data_dir, 'test', enc)\n",
        "elif FLAGS.data_name == \"multi_roc\":\n",
        "    data_train, data_dev, data_test = [], [], []\n",
        "    data_name = [\"roc\", \"roc_shuffle\", \"roc_replace\", \"roc_repeat\"]\n",
        "    assert FLAGS.n_class == len(data_name)\n",
        "    for _, name in enumerate(data_name):\n",
        "        if \"shuffle\" in name:\n",
        "            label = 1\n",
        "        elif \"replace\" in name:\n",
        "            label = 2\n",
        "        elif \"repeat\" in name:\n",
        "            label = 3\n",
        "        else:\n",
        "            label = 0\n",
        "        data = load_data(FLAGS.data_dir, name, enc, label=label)\n",
        "        data_segnum = len(data) / 20\n",
        "        data_train.extend(data[:int(data_segnum*18)])\n",
        "        data_dev.extend(data[int(data_segnum*18):int(data_segnum*19)])\n",
        "        data_test.extend(data[int(data_segnum*19):])\n",
        "else:\n",
        "    print(\"DATANAME ERROR\")\n",
        "    exit()\n",
        "random.shuffle(data_train)\n",
        "random.shuffle(data_dev)\n",
        "random.shuffle(data_test)\n",
        "print(\"Number of data for training:%d\"%len(data_train))\n",
        "print(\"Number of data for validation:%d\"%len(data_dev))\n",
        "print(\"Number of data for testing:%d\"%len(data_test))\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "with tf.Session(config=config) as sess:\n",
        "    context = tf.placeholder(tf.int32, [None, None])\n",
        "    context_length = tf.placeholder(tf.int32, [None])\n",
        "    label = tf.placeholder(tf.int32, [None])\n",
        "    \n",
        "    context_entity = tf.placeholder(tf.int32, [None, None, None, 3, None])\n",
        "    context_entity_mask = tf.placeholder(tf.int32, [None, None, None, 3, None])\n",
        "\n",
        "    epoch = tf.Variable(0, trainable=False, name='initialize/epoch')\n",
        "    epoch_add_op = epoch.assign(epoch + 1)\n",
        "    if FLAGS.data_name == \"multi_roc\":\n",
        "        model_loss, model_loss_lm, model_acc_list = train_classify(\n",
        "            hparams=hparams,\n",
        "            context=context,\n",
        "            context_length=context_length,\n",
        "            label=label,\n",
        "            enc=enc,\n",
        "            n_class=FLAGS.n_class,\n",
        "            PAD_ID=PAD_ID,\n",
        "        )\n",
        "    else:\n",
        "        model_loss = model_train(\n",
        "            hparams=hparams,\n",
        "            context=context,\n",
        "            context_length=context_length,\n",
        "            enc=enc,\n",
        "            PAD_ID=PAD_ID,\n",
        "        )\n",
        "\n",
        "    params = tf.trainable_variables()\n",
        "    for item in params:\n",
        "        print('%s: %s' % (item.name, item.get_shape()))\n",
        "\n",
        "    # initialize the training process\n",
        "    learning_rate = tf.Variable(float(FLAGS.learning_rate), trainable=False, \n",
        "            dtype=tf.float32, name=\"initialize/learning_rate\")\n",
        "    learning_rate_decay_op = learning_rate.assign(learning_rate * 0.95)\n",
        "    learning_rate_assign_op = learning_rate.assign(FLAGS.learning_rate)\n",
        "    global_step = tf.Variable(1, trainable=False, name=\"initialize/global_step\")\n",
        "\n",
        "    max_gradient_norm = 5\n",
        "    clipped_gradients, gradient_norm = tf.clip_by_global_norm(tf.gradients(model_loss, params), \n",
        "            max_gradient_norm)\n",
        "    update = tf.train.AdamOptimizer(learning_rate).apply_gradients(zip(clipped_gradients, params), global_step=global_step)\n",
        "\n",
        "    try:\n",
        "        saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2, \n",
        "                max_to_keep=10, pad_step_number=True, keep_checkpoint_every_n_hours=1.0)\n",
        "        print(\"Reading model parameters from %s\" % (train_dir))\n",
        "        saver.restore(sess, tf.train.latest_checkpoint(train_dir))\n",
        "    except:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        try:\n",
        "            restore_tensor = []\n",
        "            for tensor in tf.global_variables():\n",
        "                if \"fine_tuning\" not in tensor.name:\n",
        "                    restore_tensor.append(tensor)\n",
        "                else:\n",
        "                    print(\"to-be-initialized parameter:\", tensor.name)\n",
        "            print(\"=\"*5)\n",
        "            saver = tf.train.Saver(restore_tensor, write_version=tf.train.SaverDef.V2, \n",
        "                    max_to_keep=10, pad_step_number=True, keep_checkpoint_every_n_hours=1.0)\n",
        "            saver.restore(sess, tf.train.latest_checkpoint(train_dir))\n",
        "            print(\"Initialize the classifier parameter.\")\n",
        "        except:\n",
        "            restore_tensor = []\n",
        "            for tensor in tf.global_variables():\n",
        "                if \"beta\" not in tensor.name and \"initialize\" not in tensor.name and \"fine_tuning\" not in tensor.name and \"Adam\" not in tensor.name:\n",
        "                    restore_tensor.append(tensor)\n",
        "                else:\n",
        "                    print(\"to-be-initialized parameter:\", tensor.name)\n",
        "            print(\"=\"*5)\n",
        "            saver = tf.train.Saver(restore_tensor, write_version=tf.train.SaverDef.V2, \n",
        "                    max_to_keep=10, pad_step_number=True, keep_checkpoint_every_n_hours=1.0)\n",
        "            saver.restore(sess, tf.train.latest_checkpoint(train_dir))\n",
        "            print(\"Initialize all the fine-tuning parameter.\")\n",
        "        saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2, \n",
        "                max_to_keep=10, pad_step_number=True, keep_checkpoint_every_n_hours=1.0)\n",
        "        saver.save(sess, '%s/checkpoint' % train_dir, global_step=global_step.eval())\n",
        "        print(\"Reading model parameters from %s and initialize the parameters for fine-tuning.\" % (train_dir))\n",
        "\n",
        "    if FLAGS.is_train:\n",
        "        best_loss = 1e10\n",
        "        pre_losses = [1e18] * 3\n",
        "        while True:\n",
        "            random.shuffle(data_train)\n",
        "            start_time = time.time()\n",
        "            loss = train(sess, data_train, is_train=True)\n",
        "            if loss > max(pre_losses):  # Learning rate decay\n",
        "                sess.run(learning_rate_decay_op)\n",
        "            pre_losses = pre_losses[1:] + [loss]\n",
        "            print(\"Gen epoch %d learning rate %.4f epoch-time %.4f: \" % (epoch.eval(), learning_rate.eval(), time.time() - start_time))\n",
        "            print(\"PPL on training set:\", loss)\n",
        "            loss = train(sess, data_test, is_train=False)\n",
        "            print(\"        PPL on validation set:\", loss)\n",
        "            if loss < best_loss:\n",
        "                best_loss = loss\n",
        "                loss = train(sess, data_test, is_train=False)\n",
        "                print(\"        PPL on testing set:\", loss)\n",
        "                saver.save(sess, '%s/checkpoint' % train_dir, global_step=global_step.eval())\n",
        "                print(\"saving parameters in %s\" % train_dir)\n",
        "    else:\n",
        "        if FLAGS.cond:\n",
        "            print(\"begin conditionally generating stories......\")\n",
        "            interact_model(sess=sess,\n",
        "                            enc=enc,\n",
        "                            PAD_ID=PAD_ID,\n",
        "                            hparams=hparams,\n",
        "                            context=context,\n",
        "                            dataset=data_test, #  Accept console input if `dataset` is set to None \n",
        "                            output_file_name=\"./inference_gpt2.txt\",\n",
        "                            temperature=FLAGS.temperature,\n",
        "                            top_k=FLAGS.top_k)\n",
        "        else:\n",
        "            print(\"begin unconditionally generating stories......\")\n",
        "            sample_model(sess=sess,\n",
        "                            enc=enc,\n",
        "                            PAD_ID=PAD_ID,\n",
        "                            hparams=hparams,\n",
        "                            temperature=FLAGS.temperature,\n",
        "                            top_k=FLAGS.top_k)\n",
        "        print(\"end generating stories......\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 0-th gpu ...\n",
            "begin loading dataset......\n",
            "Number of data for training:100001\n",
            "Number of data for validation:1960\n",
            "Number of data for testing:1987\n",
            "model/wpe:0: (1024, 768)\n",
            "model/wte:0: (50257, 768)\n",
            "model/h0/ln_1/g:0: (768,)\n",
            "model/h0/ln_1/b:0: (768,)\n",
            "model/h0/attn/c_attn/w:0: (1, 768, 2304)\n",
            "model/h0/attn/c_attn/b:0: (2304,)\n",
            "model/h0/attn/c_proj/w:0: (1, 768, 768)\n",
            "model/h0/attn/c_proj/b:0: (768,)\n",
            "model/h0/ln_2/g:0: (768,)\n",
            "model/h0/ln_2/b:0: (768,)\n",
            "model/h0/mlp/c_fc/w:0: (1, 768, 3072)\n",
            "model/h0/mlp/c_fc/b:0: (3072,)\n",
            "model/h0/mlp/c_proj/w:0: (1, 3072, 768)\n",
            "model/h0/mlp/c_proj/b:0: (768,)\n",
            "model/h1/ln_1/g:0: (768,)\n",
            "model/h1/ln_1/b:0: (768,)\n",
            "model/h1/attn/c_attn/w:0: (1, 768, 2304)\n",
            "model/h1/attn/c_attn/b:0: (2304,)\n",
            "model/h1/attn/c_proj/w:0: (1, 768, 768)\n",
            "model/h1/attn/c_proj/b:0: (768,)\n",
            "model/h1/ln_2/g:0: (768,)\n",
            "model/h1/ln_2/b:0: (768,)\n",
            "model/h1/mlp/c_fc/w:0: (1, 768, 3072)\n",
            "model/h1/mlp/c_fc/b:0: (3072,)\n",
            "model/h1/mlp/c_proj/w:0: (1, 3072, 768)\n",
            "model/h1/mlp/c_proj/b:0: (768,)\n",
            "model/h2/ln_1/g:0: (768,)\n",
            "model/h2/ln_1/b:0: (768,)\n",
            "model/h2/attn/c_attn/w:0: (1, 768, 2304)\n",
            "model/h2/attn/c_attn/b:0: (2304,)\n",
            "model/h2/attn/c_proj/w:0: (1, 768, 768)\n",
            "model/h2/attn/c_proj/b:0: (768,)\n",
            "model/h2/ln_2/g:0: (768,)\n",
            "model/h2/ln_2/b:0: (768,)\n",
            "model/h2/mlp/c_fc/w:0: (1, 768, 3072)\n",
            "model/h2/mlp/c_fc/b:0: (3072,)\n",
            "model/h2/mlp/c_proj/w:0: (1, 3072, 768)\n",
            "model/h2/mlp/c_proj/b:0: (768,)\n",
            "model/h3/ln_1/g:0: (768,)\n",
            "model/h3/ln_1/b:0: (768,)\n",
            "model/h3/attn/c_attn/w:0: (1, 768, 2304)\n",
            "model/h3/attn/c_attn/b:0: (2304,)\n",
            "model/h3/attn/c_proj/w:0: (1, 768, 768)\n",
            "model/h3/attn/c_proj/b:0: (768,)\n",
            "model/h3/ln_2/g:0: (768,)\n",
            "model/h3/ln_2/b:0: (768,)\n",
            "model/h3/mlp/c_fc/w:0: (1, 768, 3072)\n",
            "model/h3/mlp/c_fc/b:0: (3072,)\n",
            "model/h3/mlp/c_proj/w:0: (1, 3072, 768)\n",
            "model/h3/mlp/c_proj/b:0: (768,)\n",
            "model/h4/ln_1/g:0: (768,)\n",
            "model/h4/ln_1/b:0: (768,)\n",
            "model/h4/attn/c_attn/w:0: (1, 768, 2304)\n",
            "model/h4/attn/c_attn/b:0: (2304,)\n",
            "model/h4/attn/c_proj/w:0: (1, 768, 768)\n",
            "model/h4/attn/c_proj/b:0: (768,)\n",
            "model/h4/ln_2/g:0: (768,)\n",
            "model/h4/ln_2/b:0: (768,)\n",
            "model/h4/mlp/c_fc/w:0: (1, 768, 3072)\n",
            "model/h4/mlp/c_fc/b:0: (3072,)\n",
            "model/h4/mlp/c_proj/w:0: (1, 3072, 768)\n",
            "model/h4/mlp/c_proj/b:0: (768,)\n",
            "model/h5/ln_1/g:0: (768,)\n",
            "model/h5/ln_1/b:0: (768,)\n",
            "model/h5/attn/c_attn/w:0: (1, 768, 2304)\n",
            "model/h5/attn/c_attn/b:0: (2304,)\n",
            "model/h5/attn/c_proj/w:0: (1, 768, 768)\n",
            "model/h5/attn/c_proj/b:0: (768,)\n",
            "model/h5/ln_2/g:0: (768,)\n",
            "model/h5/ln_2/b:0: (768,)\n",
            "model/h5/mlp/c_fc/w:0: (1, 768, 3072)\n",
            "model/h5/mlp/c_fc/b:0: (3072,)\n",
            "model/h5/mlp/c_proj/w:0: (1, 3072, 768)\n",
            "model/h5/mlp/c_proj/b:0: (768,)\n",
            "model/h6/ln_1/g:0: (768,)\n",
            "model/h6/ln_1/b:0: (768,)\n",
            "model/h6/attn/c_attn/w:0: (1, 768, 2304)\n",
            "model/h6/attn/c_attn/b:0: (2304,)\n",
            "model/h6/attn/c_proj/w:0: (1, 768, 768)\n",
            "model/h6/attn/c_proj/b:0: (768,)\n",
            "model/h6/ln_2/g:0: (768,)\n",
            "model/h6/ln_2/b:0: (768,)\n",
            "model/h6/mlp/c_fc/w:0: (1, 768, 3072)\n",
            "model/h6/mlp/c_fc/b:0: (3072,)\n",
            "model/h6/mlp/c_proj/w:0: (1, 3072, 768)\n",
            "model/h6/mlp/c_proj/b:0: (768,)\n",
            "model/h7/ln_1/g:0: (768,)\n",
            "model/h7/ln_1/b:0: (768,)\n",
            "model/h7/attn/c_attn/w:0: (1, 768, 2304)\n",
            "model/h7/attn/c_attn/b:0: (2304,)\n",
            "model/h7/attn/c_proj/w:0: (1, 768, 768)\n",
            "model/h7/attn/c_proj/b:0: (768,)\n",
            "model/h7/ln_2/g:0: (768,)\n",
            "model/h7/ln_2/b:0: (768,)\n",
            "model/h7/mlp/c_fc/w:0: (1, 768, 3072)\n",
            "model/h7/mlp/c_fc/b:0: (3072,)\n",
            "model/h7/mlp/c_proj/w:0: (1, 3072, 768)\n",
            "model/h7/mlp/c_proj/b:0: (768,)\n",
            "model/h8/ln_1/g:0: (768,)\n",
            "model/h8/ln_1/b:0: (768,)\n",
            "model/h8/attn/c_attn/w:0: (1, 768, 2304)\n",
            "model/h8/attn/c_attn/b:0: (2304,)\n",
            "model/h8/attn/c_proj/w:0: (1, 768, 768)\n",
            "model/h8/attn/c_proj/b:0: (768,)\n",
            "model/h8/ln_2/g:0: (768,)\n",
            "model/h8/ln_2/b:0: (768,)\n",
            "model/h8/mlp/c_fc/w:0: (1, 768, 3072)\n",
            "model/h8/mlp/c_fc/b:0: (3072,)\n",
            "model/h8/mlp/c_proj/w:0: (1, 3072, 768)\n",
            "model/h8/mlp/c_proj/b:0: (768,)\n",
            "model/h9/ln_1/g:0: (768,)\n",
            "model/h9/ln_1/b:0: (768,)\n",
            "model/h9/attn/c_attn/w:0: (1, 768, 2304)\n",
            "model/h9/attn/c_attn/b:0: (2304,)\n",
            "model/h9/attn/c_proj/w:0: (1, 768, 768)\n",
            "model/h9/attn/c_proj/b:0: (768,)\n",
            "model/h9/ln_2/g:0: (768,)\n",
            "model/h9/ln_2/b:0: (768,)\n",
            "model/h9/mlp/c_fc/w:0: (1, 768, 3072)\n",
            "model/h9/mlp/c_fc/b:0: (3072,)\n",
            "model/h9/mlp/c_proj/w:0: (1, 3072, 768)\n",
            "model/h9/mlp/c_proj/b:0: (768,)\n",
            "model/h10/ln_1/g:0: (768,)\n",
            "model/h10/ln_1/b:0: (768,)\n",
            "model/h10/attn/c_attn/w:0: (1, 768, 2304)\n",
            "model/h10/attn/c_attn/b:0: (2304,)\n",
            "model/h10/attn/c_proj/w:0: (1, 768, 768)\n",
            "model/h10/attn/c_proj/b:0: (768,)\n",
            "model/h10/ln_2/g:0: (768,)\n",
            "model/h10/ln_2/b:0: (768,)\n",
            "model/h10/mlp/c_fc/w:0: (1, 768, 3072)\n",
            "model/h10/mlp/c_fc/b:0: (3072,)\n",
            "model/h10/mlp/c_proj/w:0: (1, 3072, 768)\n",
            "model/h10/mlp/c_proj/b:0: (768,)\n",
            "model/h11/ln_1/g:0: (768,)\n",
            "model/h11/ln_1/b:0: (768,)\n",
            "model/h11/attn/c_attn/w:0: (1, 768, 2304)\n",
            "model/h11/attn/c_attn/b:0: (2304,)\n",
            "model/h11/attn/c_proj/w:0: (1, 768, 768)\n",
            "model/h11/attn/c_proj/b:0: (768,)\n",
            "model/h11/ln_2/g:0: (768,)\n",
            "model/h11/ln_2/b:0: (768,)\n",
            "model/h11/mlp/c_fc/w:0: (1, 768, 3072)\n",
            "model/h11/mlp/c_fc/b:0: (3072,)\n",
            "model/h11/mlp/c_proj/w:0: (1, 3072, 768)\n",
            "model/h11/mlp/c_proj/b:0: (768,)\n",
            "model/ln_f/g:0: (768,)\n",
            "model/ln_f/b:0: (768,)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Reading model parameters from models/117M\n",
            "INFO:tensorflow:Restoring parameters from models/117M/model.ckpt\n",
            "=====\n",
            "INFO:tensorflow:Restoring parameters from models/117M/model.ckpt\n",
            "to-be-initialized parameter: initialize/epoch:0\n",
            "to-be-initialized parameter: initialize/epoch_1:0\n",
            "to-be-initialized parameter: initialize/learning_rate:0\n",
            "to-be-initialized parameter: initialize/global_step:0\n",
            "to-be-initialized parameter: beta1_power:0\n",
            "to-be-initialized parameter: beta2_power:0\n",
            "to-be-initialized parameter: model/wpe/Adam:0\n",
            "to-be-initialized parameter: model/wpe/Adam_1:0\n",
            "to-be-initialized parameter: model/wte/Adam:0\n",
            "to-be-initialized parameter: model/wte/Adam_1:0\n",
            "to-be-initialized parameter: model/h0/ln_1/g/Adam:0\n",
            "to-be-initialized parameter: model/h0/ln_1/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h0/ln_1/b/Adam:0\n",
            "to-be-initialized parameter: model/h0/ln_1/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h0/attn/c_attn/w/Adam:0\n",
            "to-be-initialized parameter: model/h0/attn/c_attn/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h0/attn/c_attn/b/Adam:0\n",
            "to-be-initialized parameter: model/h0/attn/c_attn/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h0/attn/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h0/attn/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h0/attn/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h0/attn/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h0/ln_2/g/Adam:0\n",
            "to-be-initialized parameter: model/h0/ln_2/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h0/ln_2/b/Adam:0\n",
            "to-be-initialized parameter: model/h0/ln_2/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h0/mlp/c_fc/w/Adam:0\n",
            "to-be-initialized parameter: model/h0/mlp/c_fc/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h0/mlp/c_fc/b/Adam:0\n",
            "to-be-initialized parameter: model/h0/mlp/c_fc/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h0/mlp/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h0/mlp/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h0/mlp/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h0/mlp/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h1/ln_1/g/Adam:0\n",
            "to-be-initialized parameter: model/h1/ln_1/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h1/ln_1/b/Adam:0\n",
            "to-be-initialized parameter: model/h1/ln_1/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h1/attn/c_attn/w/Adam:0\n",
            "to-be-initialized parameter: model/h1/attn/c_attn/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h1/attn/c_attn/b/Adam:0\n",
            "to-be-initialized parameter: model/h1/attn/c_attn/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h1/attn/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h1/attn/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h1/attn/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h1/attn/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h1/ln_2/g/Adam:0\n",
            "to-be-initialized parameter: model/h1/ln_2/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h1/ln_2/b/Adam:0\n",
            "to-be-initialized parameter: model/h1/ln_2/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h1/mlp/c_fc/w/Adam:0\n",
            "to-be-initialized parameter: model/h1/mlp/c_fc/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h1/mlp/c_fc/b/Adam:0\n",
            "to-be-initialized parameter: model/h1/mlp/c_fc/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h1/mlp/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h1/mlp/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h1/mlp/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h1/mlp/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h2/ln_1/g/Adam:0\n",
            "to-be-initialized parameter: model/h2/ln_1/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h2/ln_1/b/Adam:0\n",
            "to-be-initialized parameter: model/h2/ln_1/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h2/attn/c_attn/w/Adam:0\n",
            "to-be-initialized parameter: model/h2/attn/c_attn/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h2/attn/c_attn/b/Adam:0\n",
            "to-be-initialized parameter: model/h2/attn/c_attn/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h2/attn/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h2/attn/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h2/attn/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h2/attn/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h2/ln_2/g/Adam:0\n",
            "to-be-initialized parameter: model/h2/ln_2/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h2/ln_2/b/Adam:0\n",
            "to-be-initialized parameter: model/h2/ln_2/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h2/mlp/c_fc/w/Adam:0\n",
            "to-be-initialized parameter: model/h2/mlp/c_fc/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h2/mlp/c_fc/b/Adam:0\n",
            "to-be-initialized parameter: model/h2/mlp/c_fc/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h2/mlp/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h2/mlp/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h2/mlp/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h2/mlp/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h3/ln_1/g/Adam:0\n",
            "to-be-initialized parameter: model/h3/ln_1/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h3/ln_1/b/Adam:0\n",
            "to-be-initialized parameter: model/h3/ln_1/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h3/attn/c_attn/w/Adam:0\n",
            "to-be-initialized parameter: model/h3/attn/c_attn/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h3/attn/c_attn/b/Adam:0\n",
            "to-be-initialized parameter: model/h3/attn/c_attn/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h3/attn/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h3/attn/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h3/attn/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h3/attn/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h3/ln_2/g/Adam:0\n",
            "to-be-initialized parameter: model/h3/ln_2/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h3/ln_2/b/Adam:0\n",
            "to-be-initialized parameter: model/h3/ln_2/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h3/mlp/c_fc/w/Adam:0\n",
            "to-be-initialized parameter: model/h3/mlp/c_fc/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h3/mlp/c_fc/b/Adam:0\n",
            "to-be-initialized parameter: model/h3/mlp/c_fc/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h3/mlp/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h3/mlp/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h3/mlp/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h3/mlp/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h4/ln_1/g/Adam:0\n",
            "to-be-initialized parameter: model/h4/ln_1/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h4/ln_1/b/Adam:0\n",
            "to-be-initialized parameter: model/h4/ln_1/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h4/attn/c_attn/w/Adam:0\n",
            "to-be-initialized parameter: model/h4/attn/c_attn/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h4/attn/c_attn/b/Adam:0\n",
            "to-be-initialized parameter: model/h4/attn/c_attn/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h4/attn/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h4/attn/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h4/attn/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h4/attn/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h4/ln_2/g/Adam:0\n",
            "to-be-initialized parameter: model/h4/ln_2/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h4/ln_2/b/Adam:0\n",
            "to-be-initialized parameter: model/h4/ln_2/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h4/mlp/c_fc/w/Adam:0\n",
            "to-be-initialized parameter: model/h4/mlp/c_fc/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h4/mlp/c_fc/b/Adam:0\n",
            "to-be-initialized parameter: model/h4/mlp/c_fc/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h4/mlp/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h4/mlp/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h4/mlp/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h4/mlp/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h5/ln_1/g/Adam:0\n",
            "to-be-initialized parameter: model/h5/ln_1/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h5/ln_1/b/Adam:0\n",
            "to-be-initialized parameter: model/h5/ln_1/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h5/attn/c_attn/w/Adam:0\n",
            "to-be-initialized parameter: model/h5/attn/c_attn/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h5/attn/c_attn/b/Adam:0\n",
            "to-be-initialized parameter: model/h5/attn/c_attn/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h5/attn/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h5/attn/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h5/attn/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h5/attn/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h5/ln_2/g/Adam:0\n",
            "to-be-initialized parameter: model/h5/ln_2/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h5/ln_2/b/Adam:0\n",
            "to-be-initialized parameter: model/h5/ln_2/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h5/mlp/c_fc/w/Adam:0\n",
            "to-be-initialized parameter: model/h5/mlp/c_fc/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h5/mlp/c_fc/b/Adam:0\n",
            "to-be-initialized parameter: model/h5/mlp/c_fc/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h5/mlp/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h5/mlp/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h5/mlp/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h5/mlp/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h6/ln_1/g/Adam:0\n",
            "to-be-initialized parameter: model/h6/ln_1/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h6/ln_1/b/Adam:0\n",
            "to-be-initialized parameter: model/h6/ln_1/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h6/attn/c_attn/w/Adam:0\n",
            "to-be-initialized parameter: model/h6/attn/c_attn/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h6/attn/c_attn/b/Adam:0\n",
            "to-be-initialized parameter: model/h6/attn/c_attn/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h6/attn/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h6/attn/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h6/attn/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h6/attn/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h6/ln_2/g/Adam:0\n",
            "to-be-initialized parameter: model/h6/ln_2/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h6/ln_2/b/Adam:0\n",
            "to-be-initialized parameter: model/h6/ln_2/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h6/mlp/c_fc/w/Adam:0\n",
            "to-be-initialized parameter: model/h6/mlp/c_fc/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h6/mlp/c_fc/b/Adam:0\n",
            "to-be-initialized parameter: model/h6/mlp/c_fc/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h6/mlp/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h6/mlp/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h6/mlp/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h6/mlp/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h7/ln_1/g/Adam:0\n",
            "to-be-initialized parameter: model/h7/ln_1/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h7/ln_1/b/Adam:0\n",
            "to-be-initialized parameter: model/h7/ln_1/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h7/attn/c_attn/w/Adam:0\n",
            "to-be-initialized parameter: model/h7/attn/c_attn/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h7/attn/c_attn/b/Adam:0\n",
            "to-be-initialized parameter: model/h7/attn/c_attn/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h7/attn/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h7/attn/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h7/attn/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h7/attn/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h7/ln_2/g/Adam:0\n",
            "to-be-initialized parameter: model/h7/ln_2/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h7/ln_2/b/Adam:0\n",
            "to-be-initialized parameter: model/h7/ln_2/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h7/mlp/c_fc/w/Adam:0\n",
            "to-be-initialized parameter: model/h7/mlp/c_fc/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h7/mlp/c_fc/b/Adam:0\n",
            "to-be-initialized parameter: model/h7/mlp/c_fc/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h7/mlp/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h7/mlp/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h7/mlp/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h7/mlp/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h8/ln_1/g/Adam:0\n",
            "to-be-initialized parameter: model/h8/ln_1/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h8/ln_1/b/Adam:0\n",
            "to-be-initialized parameter: model/h8/ln_1/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h8/attn/c_attn/w/Adam:0\n",
            "to-be-initialized parameter: model/h8/attn/c_attn/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h8/attn/c_attn/b/Adam:0\n",
            "to-be-initialized parameter: model/h8/attn/c_attn/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h8/attn/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h8/attn/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h8/attn/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h8/attn/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h8/ln_2/g/Adam:0\n",
            "to-be-initialized parameter: model/h8/ln_2/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h8/ln_2/b/Adam:0\n",
            "to-be-initialized parameter: model/h8/ln_2/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h8/mlp/c_fc/w/Adam:0\n",
            "to-be-initialized parameter: model/h8/mlp/c_fc/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h8/mlp/c_fc/b/Adam:0\n",
            "to-be-initialized parameter: model/h8/mlp/c_fc/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h8/mlp/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h8/mlp/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h8/mlp/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h8/mlp/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h9/ln_1/g/Adam:0\n",
            "to-be-initialized parameter: model/h9/ln_1/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h9/ln_1/b/Adam:0\n",
            "to-be-initialized parameter: model/h9/ln_1/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h9/attn/c_attn/w/Adam:0\n",
            "to-be-initialized parameter: model/h9/attn/c_attn/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h9/attn/c_attn/b/Adam:0\n",
            "to-be-initialized parameter: model/h9/attn/c_attn/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h9/attn/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h9/attn/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h9/attn/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h9/attn/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h9/ln_2/g/Adam:0\n",
            "to-be-initialized parameter: model/h9/ln_2/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h9/ln_2/b/Adam:0\n",
            "to-be-initialized parameter: model/h9/ln_2/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h9/mlp/c_fc/w/Adam:0\n",
            "to-be-initialized parameter: model/h9/mlp/c_fc/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h9/mlp/c_fc/b/Adam:0\n",
            "to-be-initialized parameter: model/h9/mlp/c_fc/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h9/mlp/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h9/mlp/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h9/mlp/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h9/mlp/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h10/ln_1/g/Adam:0\n",
            "to-be-initialized parameter: model/h10/ln_1/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h10/ln_1/b/Adam:0\n",
            "to-be-initialized parameter: model/h10/ln_1/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h10/attn/c_attn/w/Adam:0\n",
            "to-be-initialized parameter: model/h10/attn/c_attn/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h10/attn/c_attn/b/Adam:0\n",
            "to-be-initialized parameter: model/h10/attn/c_attn/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h10/attn/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h10/attn/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h10/attn/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h10/attn/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h10/ln_2/g/Adam:0\n",
            "to-be-initialized parameter: model/h10/ln_2/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h10/ln_2/b/Adam:0\n",
            "to-be-initialized parameter: model/h10/ln_2/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h10/mlp/c_fc/w/Adam:0\n",
            "to-be-initialized parameter: model/h10/mlp/c_fc/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h10/mlp/c_fc/b/Adam:0\n",
            "to-be-initialized parameter: model/h10/mlp/c_fc/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h10/mlp/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h10/mlp/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h10/mlp/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h10/mlp/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h11/ln_1/g/Adam:0\n",
            "to-be-initialized parameter: model/h11/ln_1/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h11/ln_1/b/Adam:0\n",
            "to-be-initialized parameter: model/h11/ln_1/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h11/attn/c_attn/w/Adam:0\n",
            "to-be-initialized parameter: model/h11/attn/c_attn/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h11/attn/c_attn/b/Adam:0\n",
            "to-be-initialized parameter: model/h11/attn/c_attn/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h11/attn/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h11/attn/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h11/attn/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h11/attn/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h11/ln_2/g/Adam:0\n",
            "to-be-initialized parameter: model/h11/ln_2/g/Adam_1:0\n",
            "to-be-initialized parameter: model/h11/ln_2/b/Adam:0\n",
            "to-be-initialized parameter: model/h11/ln_2/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h11/mlp/c_fc/w/Adam:0\n",
            "to-be-initialized parameter: model/h11/mlp/c_fc/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h11/mlp/c_fc/b/Adam:0\n",
            "to-be-initialized parameter: model/h11/mlp/c_fc/b/Adam_1:0\n",
            "to-be-initialized parameter: model/h11/mlp/c_proj/w/Adam:0\n",
            "to-be-initialized parameter: model/h11/mlp/c_proj/w/Adam_1:0\n",
            "to-be-initialized parameter: model/h11/mlp/c_proj/b/Adam:0\n",
            "to-be-initialized parameter: model/h11/mlp/c_proj/b/Adam_1:0\n",
            "to-be-initialized parameter: model/ln_f/g/Adam:0\n",
            "to-be-initialized parameter: model/ln_f/g/Adam_1:0\n",
            "to-be-initialized parameter: model/ln_f/b/Adam:0\n",
            "to-be-initialized parameter: model/ln_f/b/Adam_1:0\n",
            "=====\n",
            "INFO:tensorflow:Restoring parameters from models/117M/model.ckpt\n",
            "Initialize all the fine-tuning parameter.\n",
            "Reading model parameters from models/117M and initialize the parameters for fine-tuning.\n",
            "Gen epoch 1 learning rate 0.0001 epoch-time 970.4266: \n",
            "PPL on training set: 16.646534\n",
            "        PPL on validation set: 32.72957\n",
            "        PPL on testing set: 32.72957\n",
            "saving parameters in models/117M\n",
            "Gen epoch 2 learning rate 0.0001 epoch-time 957.5098: \n",
            "PPL on training set: 11.147687\n",
            "        PPL on validation set: 34.497913\n",
            "Gen epoch 3 learning rate 0.0001 epoch-time 956.9551: \n",
            "PPL on training set: 8.523366\n",
            "        PPL on validation set: 38.25179\n",
            "Gen epoch 4 learning rate 0.0001 epoch-time 958.6350: \n",
            "PPL on training set: 6.760943\n",
            "        PPL on validation set: 45.566578\n",
            "Gen epoch 5 learning rate 0.0001 epoch-time 959.4194: \n",
            "PPL on training set: 5.714364\n",
            "        PPL on validation set: 53.196804\n",
            "Gen epoch 6 learning rate 0.0001 epoch-time 963.9507: \n",
            "PPL on training set: 5.160819\n",
            "        PPL on validation set: 63.792908\n",
            "Gen epoch 7 learning rate 0.0001 epoch-time 958.6352: \n",
            "PPL on training set: 4.8562655\n",
            "        PPL on validation set: 66.37377\n",
            "Gen epoch 8 learning rate 0.0001 epoch-time 958.7970: \n",
            "PPL on training set: 4.67299\n",
            "        PPL on validation set: 74.37907\n",
            "Gen epoch 9 learning rate 0.0001 epoch-time 963.2732: \n",
            "PPL on training set: 4.542168\n",
            "        PPL on validation set: 76.12222\n",
            "Gen epoch 10 learning rate 0.0001 epoch-time 964.4707: \n",
            "PPL on training set: 4.44156\n",
            "        PPL on validation set: 73.16715\n",
            "Gen epoch 11 learning rate 0.0001 epoch-time 964.9954: \n",
            "PPL on training set: 4.368762\n",
            "        PPL on validation set: 77.45009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6e77801d56c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Learning rate decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate_decay_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-6e77801d56c6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, dataset, is_train)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DATANAME ERROR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_roc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}